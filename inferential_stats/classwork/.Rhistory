tm_map(stripWhitespace)
tdm = TermDocumentMatrix(cleancorp)
tdm
tdmmat = as.matrix(tdm)
# Vis
allwrds = rowSums(tdmmat)
allwrds = subset(allwrds,allwrds>=5)
freqs = rowSums(tdmmat)
freqs = subset(allwrds,freqs>=25)
freqs
View(freqs)
class(freqs)
freqs[1]
as.tibble(freqs)
as_tibble(freqs)
freqs[1][1]
# Create readable table of most frequent words
freqs %>%
group_by(freqs) %>%
summarize(freq=n()) %>%
arrange(desc(freq))
# Create readable table of most frequent words
as_tibble(freqs) %>%
group_by(freqs) %>%
summarize(freq=n()) %>%
arrange(desc(freq))
as.table(freqs[1:6])
as.table(freqs[1:6][1])
as.table(freqs[1:6][1][1])
as.table(freqs[1])
as.table(freqs[3])
as_tibble(freqs)
View(as_tibble(freqs))
setwd("C:/Users/epicm/Documents/School/flder/ISWR")
getwd
getwd()
setwd("./classwork")
#setwd("C:/Users/epicm/Documents/School/flder/ISWR")
setwd("dsprojs/inferential_stats/classwork")
#setwd("C:/Users/epicm/Documents/School/flder/ISWR")
setwd("C:/Users/epicm/Documents/School/githubdsprojs/inferential_stats/classwork")
getwd()
#setwd("C:/Users/epicm/Documents/School/flder/ISWR")
setwd("C:/Users/epicm/Documents/School/github/dsprojs/inferential_stats/classwork")
wthr = read_csv("../data/portlandWeather-NWS.txt")
wthr = read_csv("./data/portlandWeather-NWS.txt")
wthr = readcsv("./data/portlandWeather-NWS.txt")
wthr = read.csv("./data/portlandWeather-NWS.txt")
wthr = read_csv("./data/portlandWeather-NWS.txt")
library(tidyverse)
library(skimr)
wthr = read_csv("./data/portlandWeather-NWS.txt")
wthr = read_csv("../data/portlandWeather-NWS.txt")
tibble(x=1:6,theProb=1/6, themean=3.5) %>%
mutate(sqDist=(x-3.5)^2),sqDist*theProb))
tibble(x=1:6,theProb=1/6, themean=3.5) %>%
mutate((sqDist=(x-3.5)^2),sqDist*theProb)
#setwd("C:/Users/epicm/Documents/School/flder/ISWR")
library(tidyverse)
library(skimr)
wthr = read_csv("../data/portlandWeather-NWS.txt")
head(wthr)
tail(wthr)
skim(wthr)
wthr1 = wthr %>% select(theDate,maxTempF)
wthr1 = wthr1 %>% mutate(maxTempC=5/9*(maxTempF-32)) %>%
summarize(expF=mean(maxTempF,na.rm=T),expC=mean(maxTempC,na.rm=T))
wthr1
tibble(x=1:6,theProb=1/6, themean=3.5) %>%
mutate((sqDist=(x-3.5)^2),sqDist*theProb)
##wthr1 = wthr1 %>%
##  mutate(maxTempC=5/9*(maxTempF-32)) %>%
wthr1 = wthr1 %>% mutate(maxTempC=5/9*(maxTemp-32)) %>%
select(theDate,maxTempF,maxTempC) %>%
gather(key="tempUnit",value="maxTemp",
maxTempF,maxTempC)
#setwd("C:/Users/epicm/Documents/School/flder/ISWR")
library(tidyverse)
library(skimr)
wthr = read_csv("../data/portlandWeather-NWS.txt")
head(wthr)
tail(wthr)
skim(wthr)
wthr1 = wthr %>% select(theDate,maxTempF)
wthr1 = wthr1 %>% mutate(maxTempC=5/9*(maxTempF-32)) %>%
summarize(expF=mean(maxTempF,na.rm=T),expC=mean(maxTempC,na.rm=T))
wthr1
tibble(x=1:6,theProb=1/6, themean=3.5) %>%
mutate((sqDist=(x-3.5)^2),sqDist*theProb)
##wthr1 = wthr1 %>%
##  mutate(maxTempC=5/9*(maxTempF-32)) %>%
wthr1 = wthr1 %>% mutate(maxTempC=5/9*(maxTempF-32)) %>%
select(theDate,maxTempF,maxTempC) %>%
gather(key="tempUnit",value="maxTemp",
maxTempF,maxTempC)
wthr1 = wthr1 %>% mutate(maxTempC=5/9*(maxTempF-32)) %>%
select(theDate,maxTempF,maxTempC) %>%
gather(key="tempUnit",value="maxTemp",
maxTempF,maxTempC)
#setwd("C:/Users/epicm/Documents/School/flder/ISWR")
library(tidyverse)
library(skimr)
wthr = read_csv("../data/portlandWeather-NWS.txt")
head(wthr)
tail(wthr)
skim(wthr)
wthr1 = wthr %>% select(theDate,maxTempF)
wthr1 = wthr1 %>% mutate(maxTempC=5/9*(maxTempF-32)) %>%
summarize(expF=mean(maxTempF,na.rm=T),expC=mean(maxTempC,na.rm=T))
wthr1
tibble(x=1:6,theProb=1/6, themean=3.5) %>%
mutate((sqDist=(x-3.5)^2),sqDist*theProb)
wthr1 = wthr1 %>%
mutate(maxTempC=5/9*(maxTempF-32)) %>%
wthr1 = wthr1 %>% mutate(maxTempC=5/9*(maxTempF-32)) %>%
select(theDate,maxTempF,maxTempC) %>%
gather(key="tempUnit",value="maxTemp",
maxTempF,maxTempC)
tibble(dagger1=1:4,theProb=0.25,theMean=2.5) %>%
mutate(sqDist=(dagger1-theMean)^2,sqDistProb=sqDist*theProb) %>%
summarize(theVar=sum(sqDistProb),theSD=sqrt(theVar))
wthr1 = wthr1 %>%
mutate(maxTempC=5/9*(maxTempF-32)) %>%
wthr1 = wthr1 %>% mutate(maxTempC=5/9*(maxTemp-32)) %>%
select(theDate,maxTempF,maxTempC) %>%
gather(key="tempUnit",value="maxTemp",
maxTempF,maxTempC)
wthr1
#setwd("C:/Users/epicm/Documents/School/flder/ISWR")
library(tidyverse)
library(skimr)
wthr = read_csv("../data/portlandWeather-NWS.txt")
head(wthr)
tail(wthr)
skim(wthr)
wthr1 = wthr %>% select(theDate,maxTempF)
wthr1 = wthr1 %>% mutate(maxTempC=5/9*(maxTempF-32)) %>%
summarize(expF=mean(maxTempF,na.rm=T),expC=mean(maxTempC,na.rm=T))
wthr1
tibble(x=1:6,theProb=1/6, themean=3.5) %>%
mutate((sqDist=(x-3.5)^2),sqDist*theProb)
wthr1 = wthr1 %>%
mutate(maxTempC=5/9*(maxTempF-32))
#setwd("C:/Users/epicm/Documents/School/flder/ISWR")
library(tidyverse)
library(skimr)
wthr = read_csv("../data/portlandWeather-NWS.txt")
head(wthr)
tail(wthr)
skim(wthr)
wthr1 = wthr %>% select(theDate,maxTempF)
wthr1 = wthr1 %>% mutate(maxTempC=5/9*(maxTempF-32)) %>%
summarize(expF=mean(maxTempF,na.rm=T),expC=mean(maxTempC,na.rm=T))
wthr1
tibble(x=1:6,theProb=1/6, themean=3.5) %>%
mutate((sqDist=(x-3.5)^2),sqDist*theProb)
wthr2 = wthr1 %>%
mutate(maxTempC=5/9*(maxTempF-32))
#setwd("C:/Users/epicm/Documents/School/flder/ISWR")
library(tidyverse)
library(skimr)
wthr = read_csv("../data/portlandWeather-NWS.txt")
head(wthr)
tail(wthr)
skim(wthr)
wthr1 = wthr %>% select(theDate,maxTempF)
wthr2 = wthr1 %>% mutate(maxTempC=5/9*(maxTempF-32)) %>%
summarize(expF=mean(maxTempF,na.rm=T),expC=mean(maxTempC,na.rm=T))
wthr1
tibble(x=1:6,theProb=1/6, themean=3.5) %>%
mutate((sqDist=(x-3.5)^2),sqDist*theProb)
wthr3 = wthr1 %>%
mutate(maxTempC=5/9*(maxTempF-32))
wthr4 = wthr1 %>% mutate(maxTempC=5/9*(maxTempF-32)) %>%
select(theDate,maxTempF,maxTempC) %>%
gather(key="tempUnit",value="maxTemp",
maxTempF,maxTempC)
tibble(dagger1=1:4,theProb=0.25,theMean=2.5) %>%
mutate(sqDist=(dagger1-theMean)^2,sqDistProb=sqDist*theProb) %>%
summarize(theVar=sum(sqDistProb),theSD=sqrt(theVar))
#setwd("C:/Users/epicm/Documents/School/flder/ISWR/data")
library(tidyverse)
library(skimr)
shotsMade = rbinom(3141592,10,0.372)
shotsMade
tibble(shotsMade) %>%
summarize(simProb=(n()/shotsMade)) %>%
mutate(actualProb=dbinom(shotsMade,10,0.372))%>%
ggplot(aes(x=shotsMade)) +
geom_bar(fill="pink",color="black") +
scale_x_continuous(n.breaks=11)
## example
ppg = rpois(3141592,31)
tibble(ppg)%>%
ggplot() +
geom_bar(aes(x=ppg), fill="cyan4",color="black")+
scale_x_continuous(n.breaks=max(ppg)/2) +
labs(title="Damian Lillard PPG", x="ppg") +
theme_minimal()
## exercise #1
notds=read_csv("../data/SATscores-2023.txt")
head(notds)
tail(notds)
names(notds)
#ds %>%
#  select()
skim(notds)
notds%>%
arrange(desc(averageSatMath)) %>%
print(n=28)
notds%>%
summarize(
minSAT=min(averageSatMath),
Q1=quantile(averageSatMath,0.25),
medSAT=median(averageSatMath),
Q3=quantile(averageSatMath,0.75),
maxSAT=max(averageSatMath)
)
notds%>%
ggplot(aes(x=averageSatMath))+
geom_histogram(color="black",fill="salmon",bins=12)+
geom_vline(xintercept=548)+
ggtitle("State-level SAT data 2023") + theme_minimal()
## exercise #2
ds=read_csv("../data/ACTscores-2023.txt")
head(ds)
tail(ds)
names(ds)
skim(ds)
ds%>%
filter(state=="Oregon")
ds%>%
arrange(desc(averageACTScore))
ds%>%
summarize(
minSAT=min(averageACTScore),
Q1=quantile(averageACTScore,0.25),
medSAT=median(averageACTScore),
Q3=quantile(averageACTScore,0.75),
maxSAT=max(averageACTScore)
)
ds%>%
ggplot(aes(x=averageACTScore))+
geom_histogram(color="black",fill="maroon",bins=12)+
geom_vline(xintercept=21)+
ggtitle("State-level ACT data 2023") + theme_minimal()
## standardizing
notds%>%
mutate(zmath=(averageSatMath-mean(averageSatMath))/sd(averageSatMath)
)%>%
select(state,averageSatMath,zmath) %>%
print(n=30)
library(tidyverse)
#setwd('C:\\Users\\epicm\\Documents\\School\\flder\\ISWR\\data')
ds = read_csv("../data/foodSpending.csv")
theMean=mean(ds$amt)
theSD=sd(ds$amt)
n=20
MOE=qt(0.975,n-1)*theSD/sqrt(n)
MOE
lower=theMean-MOE
upper=theMean+MOE
lower
upper
ds %>%
summarize(theMean=mean(amt),theSD=sd(amt),n=n(),
MOE=qt(0.975,n-1)*theSD/sqrt(n),
lower=theMean-MOE,
upper=theMean+MOE)
#Class exercise
#interpretation of a calculation of 95% confidence interval
#based on 30 responses being ($628,$1471) is that the values of
#$628 and $1471 are the bounds of this interval which contains 95% of the data.
(1471+628)/2
1471+1049.5
MOEcw = 1471-1049.5
#421.5 = qt(0.975,29)(5/sqrt(30))
#data is probably skewed if there is a high standard deviation
#presidential election poll example
candidate = c("trump","desantis","haley","pence")
prop=c(0.43,0.41,0.06,0.04)
tib = tibble(candidate,prop)
library(tidyverse)
#presidential election poll example
library(tidyverse)
candidate = c("trump","desantis","haley","pence")
prop=c(0.43,0.41,0.06,0.04)
tib = tibble(candidate,prop)
n=1580 #people surveyed
tib %>%
mutate(MOE=qnorm(0.975)*sqrt(prop*(1-prop)/n),
lower=prop-MOE,
upper=prop+MOE)
#class exercise
prop=295/1005 #prop of adults with tattoos.
n=1005
#99% confidence
MOE=qnorm(0.995)*sqrt(prop*(1-prop)/n)
upper=prop+MOE
lower=prop-MOE
#95% confidence
MOE=qnorm(0.975)*sqrt(prop*(1-prop)/n)
upper=prop+MOE
lower=prop-MOE
#90% confidence
MOE=qnorm(0.95)*sqrt(prop*(1-prop)/n)
upper=prop+MOE
lower=prop-MOE
tibble(prop,confCoeff=qnorm(c(0.95,0.975,0.995))) %>%
mutate(MOE=confCoeff*sqrt(prop*(1-prop)/n),
lower=prop-MOE,
upper=prop+MOE)
#95% confident that the true proportion of adults with a tattoo or more
#is captured by the interval (27.0%,31.7%)
#Class exercise:
n = 801
Sprop = 0.38*n #% of black americans unable to afford medcare
Hprop = 0.28 #% of americans unable to afford medcare
pval = 1-pnorm(Sprop,Hprop,sqrt(Hprop*(1-Hprop)/n))
pval
zscore = sqrt(Hprop*(1-Hprop)/n)
zscore
x1=154
n1=438
phat1=x1/n1
x2=132
n2=389
phat2=x2/n2
phat1
phat2
samplediff=phat1-phat2
p_pool=(x1+x2)/(n1+n2)
p_pool
#sd = se
se = sqrt(p_pool*(1-p_pool)/n1 +
p_pool*(1-p_pool)/n2
)
se
pnorm(-samplediff,0,se)
pnorm(samplediff,0,se,lower.tail=F)
#x=counts
#n=sample size
#alternative = sign of alternative hypothesis
prop.test(x=c(x1,x2),n=c(n1,n2),alternative="two.sided")
#**********************#
#class ex
x1=round(0.080*11545,digits=3)
x2=0.088*4691
n1=11545
n2=4691
prop.test(x=c(x1,x2),n=c(n1,n2),alternative="two.sided")
#Class example (politics on 10; out of order)
nbe=630
phatbe=0.54
xbe=nbe*phatbe
naf=1010
phataf=0.51
xaf=naf*phataf
prop.test(x=c(xbe,xaf),
n=c(nbe,naf),
alternative="greater")
#disgusting variable names because lmao tryna catch up
library(tidyverse)
library(skimr)
mu=1000
x=c(1214,1087,1200,1419,1121,1325,1345,1244,1258,1356,1132,1191,1270,1295,1135)
skim(x)
# or
tibble(x=x) %>%
summarize(sampleMean=mean(x), sampleSD=sd(x), sampleSize=n())
#t-testing
#*Params:
#*x
#*hypothesized mean
#*alternative hypothesis
t.test(x,mu=1000,alternative="less")
tibble(x=x) %>%
ggplot() +
geom_histogram(aes(x=x),fill="seagreen4",color="black",bins=10) +
labs(title="Chips Ahoy! Experiment", x="#chips") +
theme_minimal()
#setwd("C:\\Users\\epicm\\Documents\\School\\20222023\\ISWR\\data")
df = read_csv("../data/tires.csv")
#class exercise
#assumption checking
#*observations are independent
#*sample is representative of population (random)
#*normal?
df
df %>%
ggplot()+
labs(title="Tires survey",x="stopping distance") +
theme_minimal()
#*normal indeed
t.test(df,mu=125,alternative="greater")
## ----------------
library(tidyverse)
library(skimr)
#setwd("C:\\Users\\epicm\\Documents\\School\\20222023\\ISWR\\data")
ds=read_csv("../data/textbooks.csv")
skim(ds)
t.test(ds$diff,mu=0,alternative="")
?t.test()
## ----------------
library(tidyverse)
library(skimr)
#setwd("C:\\Users\\epicm\\Documents\\School\\20222023\\ISWR\\data")
ds=read_csv("../data/textbooks.csv")
skim(ds)
t.test(ds$diff,mu=0,alternative="two.sided")
t.test(ds$ucla_new,ds$amaz_new,paired=T,alternative="less")
#baseball example
t.test(baseball$diff,mu=0,alternative="greater")
## ----------------
library(tidyverse)
library(skimr)
#setwd("C:\\Users\\epicm\\Documents\\School\\20222023\\ISWR\\data")
ds=read_csv("../data/textbooks.csv")
skim(ds)
t.test(ds$diff,mu=0,alternative="two.sided")
t.test(ds$ucla_new,ds$amaz_new,paired=T,alternative="less")
#baseball example
t.test(baseball$diff,mu=0,alternative="greater")
#baseball example
t.test(ds$baseball$diff,mu=0,alternative="greater")
#baseball example
t.test(ds$diff,mu=0,alternative="greater")
## ----------------
library(tidyverse)
library(skimr)
#setwd("C:\\Users\\epicm\\Documents\\School\\20222023\\ISWR\\data")
ds=read_csv("../data/textbooks.csv")
skim(ds)
t.test(ds$diff,mu=0,alternative="two.sided")
t.test(ds$ucla_new,ds$amaz_new,paired=T,alternative="less")
#baseball example
t.test(ds$diff,mu=0,alternative="greater")
#* issues with these conclusions
#* - sample size
#* - representation is shaky
#* - prior experience, skill, training duration?
#* - pre v. post where even little variation
#* in performance affects conclusion of the study
#*
## Class exercise
getwd()
#setwd("C:\\Users\\epicm\\Documents\\School\\20222023\\ISWR\\slides\\14 - Regression")
library(tidyverse)
library(skimr)
ds = read_table("../data/Hand_dexterity.txt")
## Class exercise
getwd()
#setwd("C:\\Users\\epicm\\Documents\\School\\20222023\\ISWR\\slides\\14 - Regression")
library(tidyverse)
library(skimr)
ds = read_table("../data/Hand_dexterity.txt")
library(tidyverse)
library(skimr)
library(car)
#setwd("C:\\Users\\epicm\\Documents\\School\\20222023\\ISWR\\data")
bs=read_tsv("../data/house_prices_tab.txt")
bs %>%
ggplot(aes(x=sqft_living,y=price)) +
geom_point(aes(color=purple)) +
geom_smooth(method="lm") + #shows a violation of EqualVariance
theme_minimal()
library(tidyverse)
library(skimr)
library(car)
#setwd("C:\\Users\\epicm\\Documents\\School\\20222023\\ISWR\\data")
bs=read_tsv("../data/house_prices_tab.txt")
bs %>%
ggplot(aes(x=sqft_living,y=price)) +
geom_point(aes(color="purple")) +
geom_smooth(method="lm") + #shows a violation of EqualVariance
theme_minimal()
mod=lm(price~sqft_living, data=bs)
summary(mod)
plot(mod)
#"different from zero" Qanswer: p value showing significance
# of difference from being a non-zero slope (Null hypoth)
#first plot
bs %>%
ggplot(aes(x=sqft_living,y=price,color=condition)) +
geom_point()
#second plot
bs %>%
ggplot(aes(x=sqft_living,y=price,color=condition)) +
geom_point()+
scale_x_continuous(trans="log")
#third plot
bs %>%
ggplot(aes(x=sqft_living,y=price,color=factor(condition))) +
geom_point(alpha=0.3)+
scale_x_continuous(trans="log")+
scale_y_continuous(trans="log")+
theme_minimal()
#fourth plot
bs %>%
ggplot(aes(x=sqft_living,y=price,color=factor(condition))) +
geom_point(alpha=0.3)+
scale_x_continuous(trans="log")+
scale_y_continuous(trans="log")+
geom_smooth(method="lm")+
theme_minimal()
bs%>%mutate(condition=factor(condition))
mod2=lm(log(price ~ ( log(sqft_living + condition) )+ #(this part is unnecessary)
log(sqft_living)*condition, data=ds))
mod2=lm(log(price ~ ( log(sqft_living + condition) )+ #(this part is unnecessary)
log(sqft_living)*condition, data=bs))
mod2=lm(log(price ~ ( log(sqft_living + condition) )+ #(this part is unnecessary)
log(sqft_living)*condition, data=bs))
